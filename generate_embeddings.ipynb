{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69161000",
   "metadata": {},
   "source": [
    "# Paper Embedding Generation Pipeline\n",
    "\n",
    "This notebook generates dense vector embeddings for academic papers using the SPECTER2 model. The embeddings capture semantic similarity between papers based on their titles and abstracts, enabling downstream tasks and analysis.\n",
    "\n",
    "**Model:** [SPECTER2](https://huggingface.co/allenai/specter2) - A transformer-based model from Allen AI, specifically trained on scientific documents for document-level representation learning.\n",
    "\n",
    "**Input:**\n",
    "- `S2_papers_cleaned.db` - Cleaned Semantic Scholar papers with titles and abstracts (from `clean_and_merge_dbs.ipynb`) or downloadable from Hugging Face Hub: [`lalit3c/S2_CS_PHY_PYSCH_papers`](https://huggingface.co/datasets/lalit3c/S2_CS_PHY_PYSCH_papers)\n",
    "\n",
    "**Output:**\n",
    "- Incremental uploads to Hugging Face Hub: [`lalit3c/S2_CS_PHY_PYSCH_papers`](https://huggingface.co/datasets/lalit3c/S2_CS_PHY_PYSCH_papers)\n",
    "\n",
    "**Note**\n",
    "- Used GPU device for speedup\n",
    "- Incremental uploads to Hugging Face Hub for fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683cd965-8c5e-4fcd-b95a-1fc289e31c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers adapters huggingface_hub duckdb pandas numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d64e441c-3508-4ace-8e5a-7aa205a0cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA L40S\n",
      "Total GPU memory: 44.52056884765625 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Total GPU memory:\",\n",
    "          torch.cuda.get_device_properties(0).total_memory / 1024**3, \"GB\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a98c5d7-ada5-4e0c-a0c8-2af967e1f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import duckdb\n",
    "import pandas\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b111466-a8da-4c6b-87f2-85c2e0a0a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "from huggingface_hub import login, hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30b1fb",
   "metadata": {},
   "source": [
    "##  Data and Model Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ce715-37a2-4a36-a610-b6101fcf7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token='') #Hugging Face token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e597aad",
   "metadata": {},
   "source": [
    "### Download Papers Database\n",
    "\n",
    "Download the cleaned papers database from Hugging Face Hub containing titles and abstracts for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff8504-3bf8-4a73-abc2-68dd3ff3759f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0d79a9c9554717906eb20542c9757a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2_papers_cleaned_additional_papers.db:   0%|          | 0.00/283M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_filename = hf_hub_download(\n",
    "    repo_id=\"lalit3c/S2_CS_PHY_PYSCH_papers\",\n",
    "    repo_type=\"dataset\",  # Important: specify it's a dataset repo\n",
    "    filename='S2_papers_cleaned.db', \n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57d20833-b7ea-42c4-915e-00ab4e6e91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_db = duckdb.connect(local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c835f13",
   "metadata": {},
   "source": [
    "### Load SPECTER2 Model\n",
    "\n",
    "Load the SPECTER2 base model and its proximity adapter. SPECTER2 is designed for scientific document embeddings and uses a SciBERT backbone with task-specific adapters.\n",
    "\n",
    "**Model Architecture:**\n",
    "- Base Model: `allenai/specter2_base` (SciBERT-based)\n",
    "- Adapter: `allenai/specter2` (proximity task adapter)\n",
    "- Embedding Dimension: 768\n",
    "- Max Sequence Length: 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10b62652-c0cc-4066-b3cc-53ada3dc7daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff870e4727e44c94805244564731fd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa14835b0cb4d6ba1d9f89e64584d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa476116417d4a84a0c1571e5ced340d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6f55bf57134b08b76bcee3139a5155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cde1551b4a44341b12e3900ec1c3293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/754 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2439b8fec0f644b5b94cb387a1dfc860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0734c38b6bb34936aab65a27cf1e06d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7009b9cbed0b42bb853eff20a0e9e4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54886fa03164cb9894371df82b9d0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af7688263b746f6b2adf87778754c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae279cfa18d4f2b87d8747851001ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_adapter.bin:   0%|          | 0.00/3.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aebbee96f74e4db31acaf013b603d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertAdapterModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttentionWithAdapters(\n",
       "              (query): LoRALinearTorch(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (shared_parameters): ModuleDict()\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (key): LoRALinearTorch(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (shared_parameters): ModuleDict()\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (value): LoRALinearTorch(\n",
       "                in_features=768, out_features=768, bias=True\n",
       "                (shared_parameters): ModuleDict()\n",
       "                (loras): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningLayer(\n",
       "                (prefix_gates): ModuleDict()\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): BertSelfOutputWithAdapters(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): LoRALinearTorch(\n",
       "              in_features=768, out_features=3072, bias=True\n",
       "              (shared_parameters): ModuleDict()\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutputWithAdapters(\n",
       "            (dense): LoRALinearTorch(\n",
       "              in_features=3072, out_features=768, bias=True\n",
       "              (shared_parameters): ModuleDict()\n",
       "              (loras): ModuleDict()\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (proximity): Adapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "          (reft_layer): ReftLayer(\n",
       "            (refts): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "    (prompt_tuning): PromptTuningLayer(\n",
       "      (base_model_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (prompt_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load model and adapter ---\n",
    "hf_model_name = \"allenai/specter2_base\"\n",
    "adapter_name = \"allenai/specter2\"\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(hf_model_name)\n",
    "model.load_adapter(adapter_name, source=\"hf\", load_as=\"proximity\", set_active=True)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82b28a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c5d0a3a",
   "metadata": {},
   "source": [
    "##  Embeddings Database Setup\n",
    "\n",
    "Initialize the DuckDB database to store paper embeddings. Each embedding is a 768-dimensional float vector associated with a paper's `corpusid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51ae9f4f-69a2-4b2d-8472-6714fb9f9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single database for all embeddings\n",
    "embeddings_db = duckdb.connect('all_embeddings.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6792ab5-3b49-43c4-b4d1-299bfa4949e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x7f460008ce30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_db.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "        corpusid VARCHAR PRIMARY KEY,\n",
    "        embedding FLOAT[768]\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1be075a-3973-4513-818e-7ecd07c02a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already embedded: 0 papers\n"
     ]
    }
   ],
   "source": [
    "existing_count = embeddings_db.execute(\"SELECT COUNT(*) FROM embeddings\").fetchone()[0]\n",
    "print(f\"Already embedded: {existing_count:,} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c690ad5",
   "metadata": {},
   "source": [
    "## Embedding Generation Functions\n",
    "\n",
    "### Batch Embedding Function\n",
    "\n",
    "Define the core embedding function that processes papers in batches for efficient GPU utilization.\n",
    "\n",
    "**Input Format:** `\"Title [SEP] Abstract\"` - SPECTER2 expects title and abstract separated by a special token.\n",
    "\n",
    "**Processing:**\n",
    "1. Tokenize text with padding and truncation (max 512 tokens)\n",
    "2. Forward pass through SPECTER2 model\n",
    "3. Extract CLS token embedding (first token) as document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1cb15-aebd-4957-8aba-24108657df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_batch(texts, batch_size=64, pbar = None):\n",
    "    \"\"\"Embed texts in batches for efficiency\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        if pbar:\n",
    "            pbar.update(len(batch_texts))\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f48cc-cbfc-4358-baa7-ba39221655f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all papers \n",
    "total_count = cleaned_db.execute(\"SELECT COUNT(*) FROM papers_with_abstracts\").fetchone()[0]\n",
    "print(f\"Total papers to embed: {total_count:,}\")\n",
    "total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b396779",
   "metadata": {},
   "source": [
    "## Hugging Face Hub Integration\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "Set up batch sizes and upload thresholds for the embedding pipeline:\n",
    "- **UPLOAD_EVERY**: Number of embeddings before uploading to HF Hub (fault tolerance)\n",
    "- **FETCH_BATCH_SIZE**: Number of papers to fetch from source DB per iteration\n",
    "- **EMBED_BATCH_SIZE**: Number of papers to process through the model at once\n",
    "- **DB_INSERT_BATCH_SIZE**: Number of records to insert into embeddings DB at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa5ab46d-9c54-400e-a1ac-da05f47f85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2a5f7-0a60-4d1f-abc6-7fe06341f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_REPO_ID = \"lalit3c/S2_CS_PHY_PYSCH_papers\"\n",
    "HF_REPO_TYPE = \"dataset\"\n",
    "UPLOAD_EVERY = 100_000  # Upload every 100k embeddings\n",
    "FETCH_BATCH_SIZE = 10_000  # Fetch from DB\n",
    "EMBED_BATCH_SIZE = 64  # Process through model\n",
    "DB_INSERT_BATCH_SIZE = 10_000  # Insert to DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a59101e",
   "metadata": {},
   "source": [
    "### Upload and Cleanup Function\n",
    "\n",
    "Define helper function to upload completed embedding batches to Hugging Face Hub and clean up local storage. This enables:\n",
    "- Fault-tolerant processing (progress saved to cloud)\n",
    "- Efficient local storage usage (delete after upload)\n",
    "- Resumable processing from last successful upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625a294-73cb-4819-a8cd-f9d06b1ba466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HF API\n",
    "api = HfApi()\n",
    "\n",
    "def upload_and_cleanup(db_path, upload_count):\n",
    "    \"\"\"Upload DB to HuggingFace and clean up local file\"\"\"\n",
    "    try:\n",
    "        # Close the database connection temporarily\n",
    "        embeddings_db.close()\n",
    "        \n",
    "        # Upload to HuggingFace\n",
    "        filename = f\"embeddings/embeddings_{upload_count}.db\"\n",
    "        print(f\"\\nUploading {db_path} to HuggingFace as {filename}...\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=db_path,\n",
    "            path_in_repo=filename,\n",
    "            repo_id=HF_REPO_ID,\n",
    "            repo_type=HF_REPO_TYPE\n",
    "        )\n",
    "        print(f\"Uploaded {filename}\")\n",
    "        \n",
    "        # Delete local file to free space\n",
    "        if os.path.exists(db_path):\n",
    "            os.remove(db_path)\n",
    "            print(f\"Deleted local {db_path}\")\n",
    "        \n",
    "        # Reconnect to a new database\n",
    "        new_db = duckdb.connect(db_path)\n",
    "        new_db.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                corpusid VARCHAR PRIMARY KEY,\n",
    "                embedding FLOAT[768]\n",
    "            )\n",
    "        \"\"\")\n",
    "        return new_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error during upload/cleanup: {e}\")\n",
    "        # Reconnect even if upload failed\n",
    "        return duckdb.connect(db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f08a3",
   "metadata": {},
   "source": [
    "## Main Embedding Pipeline\n",
    "\n",
    "Execute the main embedding loop that processes all papers:\n",
    "\n",
    "1. **Fetch** papers from source database in batches\n",
    "2. **Prepare** input text as `\"Title [SEP] Abstract\"` format\n",
    "3. **Embed** batch using SPECTER2 model\n",
    "4. **Store** embeddings in local DuckDB\n",
    "5. **Upload** to Hugging Face Hub every `UPLOAD_EVERY` papers\n",
    "6. **Resume** from last checkpoint if interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb325f-9708-4dbc-96fd-6f036b6efed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process all papers in batches\n",
    "offset = existing_count\n",
    "total_processed = existing_count\n",
    "upload_counter = 0\n",
    "embeddings_since_last_upload = existing_count % UPLOAD_EVERY\n",
    "\n",
    "print(f\"Starting embedding from offset {offset:,}\")\n",
    "print(f\"Will upload every {UPLOAD_EVERY:,} embeddings\")\n",
    "\n",
    "# Create progress bar\n",
    "pbar = tqdm(total=total_count, initial=existing_count, desc=\"Embedding papers\", unit=\"papers\")\n",
    "\n",
    "while offset < total_count:\n",
    "    # Fetch batch of papers from database\n",
    "    papers_df = cleaned_db.execute(f\"\"\"\n",
    "        SELECT corpusid, title, abstract, publication_date, citation_count\n",
    "        FROM papers_with_abstracts\n",
    "        ORDER BY corpusid\n",
    "        LIMIT {FETCH_BATCH_SIZE}\n",
    "        OFFSET {offset}\n",
    "    \"\"\").df()\n",
    "    \n",
    "    if len(papers_df) == 0:\n",
    "        break\n",
    "    \n",
    "    # Prepare texts for embedding: \"Title [SEP] Abstract\"\n",
    "    texts = [f\"{row['title']} {tokenizer.sep_token} {row['abstract']}\"\n",
    "             for _, row in papers_df.iterrows()]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embed_batch(texts, batch_size=EMBED_BATCH_SIZE, pbar=pbar)\n",
    "    \n",
    "    # Prepare data for insertion\n",
    "    insert_data = []\n",
    "    for idx, (_, row) in enumerate(papers_df.iterrows()):\n",
    "        insert_data.append({\n",
    "            'corpusid': str(row['corpusid']),\n",
    "            'embedding': embeddings[idx].tolist()\n",
    "        })\n",
    "    \n",
    "    # Insert in smaller batches to avoid memory issues\n",
    "    for i in range(0, len(insert_data), DB_INSERT_BATCH_SIZE):\n",
    "        batch = insert_data[i:i + DB_INSERT_BATCH_SIZE]\n",
    "        embeddings_db.executemany(\"\"\"\n",
    "            INSERT OR REPLACE INTO embeddings\n",
    "            (corpusid, embedding)\n",
    "            VALUES (?, ?)\n",
    "        \"\"\", [(d['corpusid'], d['embedding']) for d in batch])\n",
    "    \n",
    "    total_processed += len(papers_df)\n",
    "    offset += FETCH_BATCH_SIZE\n",
    "    embeddings_since_last_upload += len(papers_df)\n",
    "    \n",
    "    # Commit periodically\n",
    "    embeddings_db.commit()\n",
    "    \n",
    "    # Update progress bar\n",
    "    pbar.set_postfix({\n",
    "        'processed': f\"{total_processed:,}\",\n",
    "        'pct': f\"{100*total_processed/total_count:.1f}%\",\n",
    "        'next_upload': f\"{UPLOAD_EVERY - embeddings_since_last_upload:,}\"\n",
    "    })\n",
    "    \n",
    "    # Upload and cleanup every 100k embeddings\n",
    "    if embeddings_since_last_upload >= UPLOAD_EVERY:\n",
    "        upload_counter += 1\n",
    "        embeddings_db = upload_and_cleanup('all_embeddings.db', upload_counter)\n",
    "        embeddings_since_last_upload = 0\n",
    "\n",
    "# Final upload if there are remaining embeddings\n",
    "if embeddings_since_last_upload > 0:\n",
    "    upload_counter += 1\n",
    "    embeddings_db = upload_and_cleanup('all_embeddings.db', upload_counter)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nEmbedding complete! Total papers embedded: {total_processed:,}\")\n",
    "print(f\" Uploaded {upload_counter} database files to HuggingFace\")\n",
    "print(f\" Repository: https://huggingface.co/datasets/{HF_REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
